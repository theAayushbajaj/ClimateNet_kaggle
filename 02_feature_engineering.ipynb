{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the required libraries\n",
    "from IPython.display import display\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.model_selection import train_test_split, \\\n",
    "                                    StratifiedKFold\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "RANDOM_SEED = np.random.seed(0)\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1, 2, 3, 4, 5, 6]), array([7460, 7460, 7460, 7460, 3730, 7460, 3730]))\n"
     ]
    }
   ],
   "source": [
    "# Clustering\n",
    "df['lon'] = (df['lon'] + 180) % 360 - 180 # converting to 180 scale\n",
    "dbscan = DBSCAN(eps=50/6371., \n",
    "                min_samples=20, \n",
    "                algorithm='ball_tree', \n",
    "                metric='haversine').fit(np.radians(df[['lat','lon']]))\n",
    "df['region'] = dbscan.labels_\n",
    "\n",
    "print(np.unique(dbscan.labels_,return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(train, test, standardize=True):    \n",
    "    # drop duplicates\n",
    "    train.drop_duplicates(inplace=True)\n",
    "\n",
    "    # Extract year, month, and day from the 'time' column\n",
    "    train['year'] = train.index.year\n",
    "    test['year'] = test.index.year\n",
    "    train['month'] = train.index.month\n",
    "    test['month'] = test.index.month\n",
    "    train['day'] = train.index.day\n",
    "    test['day'] = test.index.day\n",
    "\n",
    "    \n",
    "    # Splitting the dataset into features (X) and target (y)\n",
    "    X_train = train.drop(columns=['Label'])\n",
    "    y_train = train['Label']\n",
    "    X_test = test\n",
    "\n",
    "    if standardize:\n",
    "        scaler = StandardScaler()\n",
    "    else:\n",
    "        scaler = MinMaxScaler()\n",
    "\n",
    "    # set aside a matrix of temporal features\n",
    "    X_train_temporal = X_train[['year', 'month', 'day']]\n",
    "    X_test_temporal = X_test[['year', 'month', 'day']]\n",
    "    # drop the temporal features from X\n",
    "    X_train = X_train.drop(columns=['year', 'month', 'day'])\n",
    "    X_test = X_test.drop(columns=['year', 'month', 'day'])\n",
    "\n",
    "    # Fit scaler on training data and transform both train and test data\n",
    "    scaler.fit(X_train)  # Fit only on training data\n",
    "    X_train_normalized = pd.DataFrame(scaler.transform(X_train), columns=X_train.columns)\n",
    "    X_test_normalized = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "\n",
    "    X_train_temporal = X_train_temporal.reset_index(drop=True)\n",
    "    X_train_normalized = X_train_normalized.reset_index(drop=True)\n",
    "\n",
    "    # concatenate X_temporal and X_normalized to get X\n",
    "    X_train = pd.concat([X_train_temporal, X_train_normalized], axis=1)\n",
    "\n",
    "    X_test_temporal = X_test_temporal.reset_index(drop=True)\n",
    "    X_test_normalized = X_test_normalized.reset_index(drop=True)\n",
    "\n",
    "    # concatenate X_temporal and X_normalized to get X\n",
    "    X_test = pd.concat([X_test_temporal, X_test_normalized], axis=1)\n",
    "\n",
    "\n",
    "    # Squaring the features in X_normalized\n",
    "    for col in X_train_normalized.columns:\n",
    "        X_train[col + '_squared'] = X_train[col] ** 2\n",
    "\n",
    "    # Transforming month and day into cyclical features\n",
    "    X_train['sin_month'] = np.sin(2 * np.pi * X_train['month'] / 12)\n",
    "    X_train['cos_month'] = np.cos(2 * np.pi * X_train['month'] / 12)\n",
    "    X_train['sin_day'] = np.sin(2 * np.pi * X_train['day'] / 30)\n",
    "    X_train['cos_day'] = np.cos(2 * np.pi * X_train['day'] / 30)\n",
    "\n",
    "    # Dropping the original month and day columns\n",
    "    X_train = X_train.drop(columns=['month', 'day', 'year'])\n",
    "    # X = X.drop(columns=['month', 'day'])\n",
    "\n",
    "    # Squaring the features in X_normalized\n",
    "    for col in X_test_normalized.columns:\n",
    "        X_test[col + '_squared'] = X_test[col] ** 2\n",
    "\n",
    "    # Transforming month and day into cyclical features\n",
    "    X_test['sin_month'] = np.sin(2 * np.pi * X_test['month'] / 12)\n",
    "    X_test['cos_month'] = np.cos(2 * np.pi * X_test['month'] / 12)\n",
    "    X_test['sin_day'] = np.sin(2 * np.pi * X_test['day'] / 30)\n",
    "    X_test['cos_day'] = np.cos(2 * np.pi * X_test['day'] / 30)\n",
    "\n",
    "    # Dropping the original month and day columns\n",
    "    X_test = X_test.drop(columns=['month', 'day', 'year'])\n",
    "    # X = X.drop(columns=['month', 'day'])\n",
    "\n",
    "    return X_train, y_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CV strategy on the basis of the 'time' column where the last year is used as the test set\n",
    "(X_train, y_train, X_val), y_val= preprocess(train=df.loc[df.index.year!=2009],\n",
    "                                      test=df.loc[df.index.year==2009].drop(columns=['Label']),\n",
    "                                    standardize=True),df.loc[df.index.year==2009,'Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'X_train' (DataFrame)\n",
      "Stored 'y_train' (Series)\n",
      "Stored 'X_val' (DataFrame)\n",
      "Stored 'y_val' (Series)\n"
     ]
    }
   ],
   "source": [
    "%store X_train\n",
    "%store y_train\n",
    "%store X_val\n",
    "%store y_val"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
