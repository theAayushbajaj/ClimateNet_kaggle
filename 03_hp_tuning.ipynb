{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "import numpy as np\n",
    "from self_learn import CustomLogisticRegression, BaymaxNet\n",
    "from sklearn.model_selection import BaseCrossValidator\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r X_train\n",
    "%store -r X_val\n",
    "%store -r y_train\n",
    "%store -r y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For baseline logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = CustomLogisticRegression(validation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCV(BaseCrossValidator):\n",
    "    def __init__(self, X_train, X_val):\n",
    "        self.X_train = X_train\n",
    "        self.X_val = X_val\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        # Generate indices for the training and validation sets\n",
    "        train_indices = np.arange(len(self.X_train))\n",
    "        val_indices = np.arange(len(self.X_train), len(self.X_train) + len(self.X_val))\n",
    "        \n",
    "        yield train_indices, val_indices\n",
    "\n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        # Returns the number of splitting iterations in the cross-validator\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the custom cross-validator\n",
    "cv = CustomCV(X_train, X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_train, X_val, y_train, y_val are your training and validation sets\n",
    "# Concatenate your training and validation sets\n",
    "X = np.concatenate((X_train, X_val), axis=0)\n",
    "y = np.concatenate((y_train, y_val), axis=0)\n",
    "\n",
    "weights = utils.get_class_weights(len(y), 3, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20999, 20)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'learning_rate': [0.1,0.01,0.001],\n",
    "    'num_epochs': [1000, 1500],\n",
    "    'regularization': ['L1','L2'],\n",
    "    'lambda_reg': [0.001, 0.01, 0.1],\n",
    "    'gamma': [2.0, 3.0],\n",
    "    'class_weights':[[0, 0, 0],[2.85176468, 7.36963696, 1.66483665]]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 29728.606900185805, Train Accuracy: 0.6883184913567313, Val Accuracy: None\n",
      "Epoch 100, Loss: 12504.119467211307, Train Accuracy: 0.7656078860898138, Val Accuracy: None\n",
      "Epoch 200, Loss: 11214.75576026576, Train Accuracy: 0.765226915567408, Val Accuracy: None\n",
      "Epoch 300, Loss: 10684.403151987179, Train Accuracy: 0.7661793418734225, Val Accuracy: None\n",
      "Epoch 400, Loss: 10382.644026816995, Train Accuracy: 0.7665126910805277, Val Accuracy: None\n",
      "Epoch 500, Loss: 10182.915061295997, Train Accuracy: 0.7671317681794371, Val Accuracy: None\n",
      "Epoch 600, Loss: 10033.175456839313, Train Accuracy: 0.7683223010619553, Val Accuracy: None\n",
      "Epoch 700, Loss: 9917.19991755185, Train Accuracy: 0.7691318634220677, Val Accuracy: None\n",
      "Epoch 800, Loss: 9824.016778417503, Train Accuracy: 0.7697033192056765, Val Accuracy: None\n",
      "Epoch 900, Loss: 9745.622112741281, Train Accuracy: 0.7704176389351874, Val Accuracy: None\n",
      "Epoch 0, Loss: 0.0009057172688226544, Train Accuracy: 0.37082718224677363, Val Accuracy: None\n",
      "Epoch 100, Loss: 0.00016273494772746925, Train Accuracy: 0.24344016381732464, Val Accuracy: None\n",
      "Epoch 200, Loss: 1.3071209625913632e-05, Train Accuracy: 0.3890661460069527, Val Accuracy: None\n",
      "Epoch 300, Loss: 6.071209625913624e-06, Train Accuracy: 0.5483594456878899, Val Accuracy: None\n",
      "Epoch 400, Loss: 6.071209625913624e-06, Train Accuracy: 0.5483594456878899, Val Accuracy: None\n",
      "Epoch 500, Loss: 6.071209625913624e-06, Train Accuracy: 0.5483594456878899, Val Accuracy: None\n",
      "Epoch 600, Loss: 6.071209625913624e-06, Train Accuracy: 0.5483594456878899, Val Accuracy: None\n",
      "Epoch 700, Loss: 6.071209625913624e-06, Train Accuracy: 0.5483594456878899, Val Accuracy: None\n",
      "Epoch 800, Loss: 6.071209625913624e-06, Train Accuracy: 0.5483594456878899, Val Accuracy: None\n",
      "Epoch 900, Loss: 6.071209625913624e-06, Train Accuracy: 0.5483594456878899, Val Accuracy: None\n",
      "Epoch 0, Loss: 30518.319955659263, Train Accuracy: 0.7466546026001238, Val Accuracy: None\n",
      "Epoch 100, Loss: 9857.061740427733, Train Accuracy: 0.7743225867898471, Val Accuracy: None\n",
      "Epoch 200, Loss: 9404.652735741525, Train Accuracy: 0.7708938520881947, Val Accuracy: None\n",
      "Epoch 300, Loss: 9158.767975532795, Train Accuracy: 0.7748464212581552, Val Accuracy: None\n",
      "Epoch 400, Loss: 9054.546674020306, Train Accuracy: 0.7750845278346588, Val Accuracy: None\n",
      "Epoch 500, Loss: 8980.54301822811, Train Accuracy: 0.7744178294204486, Val Accuracy: None\n",
      "Epoch 600, Loss: 9115.741630758434, Train Accuracy: 0.7756083623029668, Val Accuracy: None\n",
      "Epoch 700, Loss: 8876.313474137623, Train Accuracy: 0.7747987999428544, Val Accuracy: None\n",
      "Epoch 800, Loss: 8865.364565648662, Train Accuracy: 0.775560740987666, Val Accuracy: None\n",
      "Epoch 900, Loss: 8819.660830927716, Train Accuracy: 0.7738939949521406, Val Accuracy: None\n",
      "Epoch 0, Loss: 29997.62198110863, Train Accuracy: 0.745035477879899, Val Accuracy: None\n",
      "Epoch 100, Loss: 9737.248904876204, Train Accuracy: 0.7728463260155245, Val Accuracy: None\n",
      "Epoch 200, Loss: 9306.861709156703, Train Accuracy: 0.7754654983570646, Val Accuracy: None\n",
      "Epoch 300, Loss: 9114.985713403667, Train Accuracy: 0.7742273441592457, Val Accuracy: None\n",
      "Epoch 400, Loss: 9020.514565600608, Train Accuracy: 0.7745606933663508, Val Accuracy: None\n",
      "Epoch 500, Loss: 8952.707097236727, Train Accuracy: 0.7720367636554122, Val Accuracy: None\n",
      "Epoch 600, Loss: 8902.136201405274, Train Accuracy: 0.7745606933663508, Val Accuracy: None\n",
      "Epoch 700, Loss: 8871.35704871641, Train Accuracy: 0.7710367160340968, Val Accuracy: None\n",
      "Epoch 800, Loss: 8771.411661491918, Train Accuracy: 0.7732272965379304, Val Accuracy: None\n",
      "Epoch 900, Loss: 8733.416052868131, Train Accuracy: 0.7742273441592457, Val Accuracy: None\n",
      "Epoch 1000, Loss: 8746.601547716073, Train Accuracy: 0.7750369065193581, Val Accuracy: None\n",
      "Epoch 1100, Loss: 8680.452251049343, Train Accuracy: 0.7743702081051479, Val Accuracy: None\n",
      "Epoch 1200, Loss: 8679.54568571413, Train Accuracy: 0.7731796752226296, Val Accuracy: None\n",
      "Epoch 1300, Loss: 8647.92782739323, Train Accuracy: 0.7744178294204486, Val Accuracy: None\n",
      "Epoch 1400, Loss: 8683.14507858462, Train Accuracy: 0.772751083384923, Val Accuracy: None\n",
      "Epoch 0, Loss: 29166.837348264024, Train Accuracy: 0.6557931330063337, Val Accuracy: None\n",
      "Epoch 100, Loss: 12720.06540678589, Train Accuracy: 0.7677032239630459, Val Accuracy: None\n",
      "Epoch 200, Loss: 11475.259463602228, Train Accuracy: 0.7683223010619553, Val Accuracy: None\n",
      "Epoch 300, Loss: 10982.695397746978, Train Accuracy: 0.768893756845564, Val Accuracy: None\n",
      "Epoch 400, Loss: 10714.108890751419, Train Accuracy: 0.7694652126291728, Val Accuracy: None\n",
      "Epoch 500, Loss: 10542.789722455624, Train Accuracy: 0.7703700176198867, Val Accuracy: None\n",
      "Epoch 600, Loss: 10421.82639296002, Train Accuracy: 0.7709414734034954, Val Accuracy: None\n",
      "Epoch 700, Loss: 10331.09530354831, Train Accuracy: 0.7711795799799991, Val Accuracy: None\n",
      "Epoch 800, Loss: 10259.433748189886, Train Accuracy: 0.7713224439259012, Val Accuracy: None\n",
      "Epoch 900, Loss: 10201.43478648207, Train Accuracy: 0.7712748226106005, Val Accuracy: None\n",
      "Epoch 0, Loss: 29914.072461784675, Train Accuracy: 0.734177817991333, Val Accuracy: None\n",
      "Epoch 100, Loss: 17360.451324102894, Train Accuracy: 0.752226296490309, Val Accuracy: None\n",
      "Epoch 200, Loss: 16084.766302194597, Train Accuracy: 0.7531787227963236, Val Accuracy: None\n",
      "Epoch 300, Loss: 15607.375730302807, Train Accuracy: 0.7524167817515119, Val Accuracy: None\n",
      "Epoch 400, Loss: 15232.607165955209, Train Accuracy: 0.7533215867422258, Val Accuracy: None\n",
      "Epoch 500, Loss: 14972.267670407422, Train Accuracy: 0.7536073146340302, Val Accuracy: None\n",
      "Epoch 600, Loss: 14795.4340833574, Train Accuracy: 0.7531787227963236, Val Accuracy: None\n",
      "Epoch 700, Loss: 14480.811725742442, Train Accuracy: 0.7544644983094433, Val Accuracy: None\n",
      "Epoch 800, Loss: 14256.382335062326, Train Accuracy: 0.755035954093052, Val Accuracy: None\n",
      "Epoch 900, Loss: 14051.635974166966, Train Accuracy: 0.755750273822563, Val Accuracy: None\n",
      "Epoch 1000, Loss: 13949.500586473569, Train Accuracy: 0.7551788180389543, Val Accuracy: None\n",
      "Epoch 1100, Loss: 13845.613338922969, Train Accuracy: 0.755750273822563, Val Accuracy: None\n",
      "Epoch 1200, Loss: 13780.281459632572, Train Accuracy: 0.7552740606695557, Val Accuracy: None\n",
      "Epoch 1300, Loss: 13700.601690901718, Train Accuracy: 0.7562264869755703, Val Accuracy: None\n",
      "Epoch 1400, Loss: 13690.574996089948, Train Accuracy: 0.7555121672460593, Val Accuracy: None\n",
      "Epoch 0, Loss: 0.007632215602947273, Train Accuracy: 0.2745368827087004, Val Accuracy: None\n",
      "Epoch 100, Loss: 0.0006189263153302388, Train Accuracy: 0.5388828039430449, Val Accuracy: None\n",
      "Epoch 200, Loss: 0.0006189263153302388, Train Accuracy: 0.5388828039430449, Val Accuracy: None\n",
      "Epoch 300, Loss: 0.0006189263153302388, Train Accuracy: 0.5388828039430449, Val Accuracy: None\n",
      "Epoch 400, Loss: 0.0006189263153302388, Train Accuracy: 0.5388828039430449, Val Accuracy: None\n",
      "Epoch 500, Loss: 0.0006189263153302388, Train Accuracy: 0.5388828039430449, Val Accuracy: None\n",
      "Epoch 600, Loss: 0.0006189263153302388, Train Accuracy: 0.5388828039430449, Val Accuracy: None\n",
      "Epoch 700, Loss: 0.0006189263153302388, Train Accuracy: 0.5388828039430449, Val Accuracy: None\n",
      "Epoch 800, Loss: 0.0006189263153302388, Train Accuracy: 0.5388828039430449, Val Accuracy: None\n",
      "Epoch 900, Loss: 0.0006189263153302388, Train Accuracy: 0.5388828039430449, Val Accuracy: None\n",
      "Epoch 0, Loss: 6.623383972475647e-05, Train Accuracy: 0.36535073098718984, Val Accuracy: None\n",
      "Epoch 100, Loss: 6.492225689960073e-05, Train Accuracy: 0.36535073098718984, Val Accuracy: None\n",
      "Epoch 200, Loss: 6.363664644014795e-05, Train Accuracy: 0.36535073098718984, Val Accuracy: None\n",
      "Epoch 300, Loss: 6.23764940336401e-05, Train Accuracy: 0.36535073098718984, Val Accuracy: None\n",
      "Epoch 400, Loss: 6.114129555189824e-05, Train Accuracy: 0.36535073098718984, Val Accuracy: None\n",
      "Epoch 500, Loss: 5.99305568496444e-05, Train Accuracy: 0.36535073098718984, Val Accuracy: None\n",
      "Epoch 600, Loss: 5.874379356681703e-05, Train Accuracy: 0.36535073098718984, Val Accuracy: None\n",
      "Epoch 700, Loss: 5.758053093480126e-05, Train Accuracy: 0.36535073098718984, Val Accuracy: None\n",
      "Epoch 800, Loss: 5.644030358649598e-05, Train Accuracy: 0.36535073098718984, Val Accuracy: None\n",
      "Epoch 900, Loss: 5.532265537014235e-05, Train Accuracy: 0.36535073098718984, Val Accuracy: None\n",
      "Epoch 1000, Loss: 5.422713916683867e-05, Train Accuracy: 0.36535073098718984, Val Accuracy: None\n",
      "Epoch 1100, Loss: 5.315331671166895e-05, Train Accuracy: 0.36535073098718984, Val Accuracy: None\n",
      "Epoch 1200, Loss: 5.210075841837357e-05, Train Accuracy: 0.36535073098718984, Val Accuracy: None\n",
      "Epoch 1300, Loss: 5.106904320749193e-05, Train Accuracy: 0.36535073098718984, Val Accuracy: None\n",
      "Epoch 1400, Loss: 5.005775833790815e-05, Train Accuracy: 0.36535073098718984, Val Accuracy: None\n",
      "Epoch 0, Loss: 30786.90438491811, Train Accuracy: 0.6505547883232535, Val Accuracy: None\n",
      "Epoch 100, Loss: 12550.27917157881, Train Accuracy: 0.7677508452783466, Val Accuracy: None\n",
      "Epoch 200, Loss: 11249.145628285036, Train Accuracy: 0.768893756845564, Val Accuracy: None\n",
      "Epoch 300, Loss: 10715.42026577683, Train Accuracy: 0.7678937092242488, Val Accuracy: None\n",
      "Epoch 400, Loss: 10412.768257840955, Train Accuracy: 0.7686080289537597, Val Accuracy: None\n",
      "Epoch 500, Loss: 10212.074129649178, Train Accuracy: 0.7685127863231582, Val Accuracy: None\n",
      "Epoch 600, Loss: 10066.015664896184, Train Accuracy: 0.7693699699985713, Val Accuracy: None\n",
      "Epoch 700, Loss: 9952.85942615331, Train Accuracy: 0.770131911043383, Val Accuracy: None\n",
      "Epoch 800, Loss: 9861.281207795248, Train Accuracy: 0.7705605028810896, Val Accuracy: None\n",
      "Epoch 900, Loss: 9785.041039744781, Train Accuracy: 0.7708462307728939, Val Accuracy: None\n",
      "Epoch 0, Loss: 19240.76274333062, Train Accuracy: 0.35154054954997854, Val Accuracy: None\n",
      "Epoch 100, Loss: 14054.565259899828, Train Accuracy: 0.7521786751750084, Val Accuracy: None\n",
      "Epoch 200, Loss: 12288.435146540558, Train Accuracy: 0.751892947283204, Val Accuracy: None\n",
      "Epoch 300, Loss: 11207.52576696198, Train Accuracy: 0.751892947283204, Val Accuracy: None\n",
      "Epoch 400, Loss: 10460.23808016271, Train Accuracy: 0.7523691604362113, Val Accuracy: None\n",
      "Epoch 500, Loss: 9909.688902464983, Train Accuracy: 0.7581313395875994, Val Accuracy: None\n",
      "Epoch 600, Loss: 9486.04628235541, Train Accuracy: 0.7622743940187628, Val Accuracy: None\n",
      "Epoch 700, Loss: 9149.235693163579, Train Accuracy: 0.7654650221439117, Val Accuracy: None\n",
      "Epoch 800, Loss: 8874.50342615453, Train Accuracy: 0.7687985142149626, Val Accuracy: None\n",
      "Epoch 900, Loss: 8645.718984483856, Train Accuracy: 0.7699890470974808, Val Accuracy: None\n",
      "Epoch 0, Loss: 34659.18314592042, Train Accuracy: 0.7511630729398439, Val Accuracy: None\n",
      "Epoch 100, Loss: 12054.127808107858, Train Accuracy: 0.7809781949107973, Val Accuracy: None\n",
      "Epoch 200, Loss: 10879.140352803945, Train Accuracy: 0.7819895626845746, Val Accuracy: None\n",
      "Epoch 300, Loss: 10496.881247666857, Train Accuracy: 0.7802500101136778, Val Accuracy: None\n",
      "Epoch 400, Loss: 10369.816518784051, Train Accuracy: 0.7791172782070472, Val Accuracy: None\n",
      "Epoch 500, Loss: 10344.609312258774, Train Accuracy: 0.7768922691047373, Val Accuracy: None\n",
      "Epoch 600, Loss: 10513.81805958409, Train Accuracy: 0.7817872891298192, Val Accuracy: None\n",
      "Epoch 700, Loss: 10929.124475922723, Train Accuracy: 0.7826368380597921, Val Accuracy: None\n",
      "Epoch 800, Loss: 10207.221928658164, Train Accuracy: 0.7813018325984061, Val Accuracy: None\n",
      "Epoch 900, Loss: 10164.683767332986, Train Accuracy: 0.7763663578623731, Val Accuracy: None\n",
      "Random Search Best Params: {'regularization': 'L1', 'num_epochs': 1000, 'learning_rate': 0.1, 'lambda_reg': 0.001, 'gamma': 2.0, 'class_weights': [2.85176468, 7.36963696, 1.66483665]}\n"
     ]
    }
   ],
   "source": [
    "# log-uniform: understand as search over p = exp(x) by varying x\n",
    "optimizer = RandomizedSearchCV(\n",
    "    estimator=CustomLogisticRegression(validation=False),\n",
    "    param_distributions={\n",
    "        'learning_rate': [0.1,0.01,0.001],\n",
    "        'num_epochs': [1000, 1500],\n",
    "        'regularization': ['L1','L2'],\n",
    "        'lambda_reg': [0.001, 0.01, 0.1],\n",
    "        'gamma': [2.0, 3.0],\n",
    "        'class_weights':[[0, 0, 0],[2.85176468, 7.36963696, 1.66483665]]\n",
    "    },\n",
    "    n_iter=50,\n",
    "    cv=cv,\n",
    "    random_state=0,\n",
    "    refit=True,\n",
    ")\n",
    "\n",
    "# executes bayesian optimization\n",
    "_ = optimizer.fit(X_train, y_train)\n",
    "\n",
    "print(\"Random Search Best Params:\", optimizer.best_params())\n",
    "# model can be saved, used for predictions or scoring\n",
    "print(optimizer.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "job exception: 'NoneType' object has no attribute 'dot'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?trial/s, best loss=?]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'dot'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m trials \u001b[39m=\u001b[39m Trials()\n\u001b[1;32m     23\u001b[0m \u001b[39m# Run the optimization\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m best \u001b[39m=\u001b[39m fmin(\n\u001b[1;32m     25\u001b[0m     fn\u001b[39m=\u001b[39;49mobjective,\n\u001b[1;32m     26\u001b[0m     space\u001b[39m=\u001b[39;49mspace,\n\u001b[1;32m     27\u001b[0m     algo\u001b[39m=\u001b[39;49mtpe\u001b[39m.\u001b[39;49msuggest,\n\u001b[1;32m     28\u001b[0m     max_evals\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,\n\u001b[1;32m     29\u001b[0m     trials\u001b[39m=\u001b[39;49mtrials\n\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     32\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBest hyperparameters:\u001b[39m\u001b[39m\"\u001b[39m, best)\n",
      "File \u001b[0;32m~/miniconda/envs/eda/lib/python3.11/site-packages/hyperopt/fmin.py:540\u001b[0m, in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    537\u001b[0m     fn \u001b[39m=\u001b[39m __objective_fmin_wrapper(fn)\n\u001b[1;32m    539\u001b[0m \u001b[39mif\u001b[39;00m allow_trials_fmin \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(trials, \u001b[39m\"\u001b[39m\u001b[39mfmin\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 540\u001b[0m     \u001b[39mreturn\u001b[39;00m trials\u001b[39m.\u001b[39;49mfmin(\n\u001b[1;32m    541\u001b[0m         fn,\n\u001b[1;32m    542\u001b[0m         space,\n\u001b[1;32m    543\u001b[0m         algo\u001b[39m=\u001b[39;49malgo,\n\u001b[1;32m    544\u001b[0m         max_evals\u001b[39m=\u001b[39;49mmax_evals,\n\u001b[1;32m    545\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    546\u001b[0m         loss_threshold\u001b[39m=\u001b[39;49mloss_threshold,\n\u001b[1;32m    547\u001b[0m         max_queue_len\u001b[39m=\u001b[39;49mmax_queue_len,\n\u001b[1;32m    548\u001b[0m         rstate\u001b[39m=\u001b[39;49mrstate,\n\u001b[1;32m    549\u001b[0m         pass_expr_memo_ctrl\u001b[39m=\u001b[39;49mpass_expr_memo_ctrl,\n\u001b[1;32m    550\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    551\u001b[0m         catch_eval_exceptions\u001b[39m=\u001b[39;49mcatch_eval_exceptions,\n\u001b[1;32m    552\u001b[0m         return_argmin\u001b[39m=\u001b[39;49mreturn_argmin,\n\u001b[1;32m    553\u001b[0m         show_progressbar\u001b[39m=\u001b[39;49mshow_progressbar,\n\u001b[1;32m    554\u001b[0m         early_stop_fn\u001b[39m=\u001b[39;49mearly_stop_fn,\n\u001b[1;32m    555\u001b[0m         trials_save_file\u001b[39m=\u001b[39;49mtrials_save_file,\n\u001b[1;32m    556\u001b[0m     )\n\u001b[1;32m    558\u001b[0m \u001b[39mif\u001b[39;00m trials \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    559\u001b[0m     \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(trials_save_file):\n",
      "File \u001b[0;32m~/miniconda/envs/eda/lib/python3.11/site-packages/hyperopt/base.py:671\u001b[0m, in \u001b[0;36mTrials.fmin\u001b[0;34m(self, fn, space, algo, max_evals, timeout, loss_threshold, max_queue_len, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[39m# -- Stop-gap implementation!\u001b[39;00m\n\u001b[1;32m    667\u001b[0m \u001b[39m#    fmin should have been a Trials method in the first place\u001b[39;00m\n\u001b[1;32m    668\u001b[0m \u001b[39m#    but for now it's still sitting in another file.\u001b[39;00m\n\u001b[1;32m    669\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfmin\u001b[39;00m \u001b[39mimport\u001b[39;00m fmin\n\u001b[0;32m--> 671\u001b[0m \u001b[39mreturn\u001b[39;00m fmin(\n\u001b[1;32m    672\u001b[0m     fn,\n\u001b[1;32m    673\u001b[0m     space,\n\u001b[1;32m    674\u001b[0m     algo\u001b[39m=\u001b[39;49malgo,\n\u001b[1;32m    675\u001b[0m     max_evals\u001b[39m=\u001b[39;49mmax_evals,\n\u001b[1;32m    676\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    677\u001b[0m     loss_threshold\u001b[39m=\u001b[39;49mloss_threshold,\n\u001b[1;32m    678\u001b[0m     trials\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    679\u001b[0m     rstate\u001b[39m=\u001b[39;49mrstate,\n\u001b[1;32m    680\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    681\u001b[0m     max_queue_len\u001b[39m=\u001b[39;49mmax_queue_len,\n\u001b[1;32m    682\u001b[0m     allow_trials_fmin\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,  \u001b[39m# -- prevent recursion\u001b[39;49;00m\n\u001b[1;32m    683\u001b[0m     pass_expr_memo_ctrl\u001b[39m=\u001b[39;49mpass_expr_memo_ctrl,\n\u001b[1;32m    684\u001b[0m     catch_eval_exceptions\u001b[39m=\u001b[39;49mcatch_eval_exceptions,\n\u001b[1;32m    685\u001b[0m     return_argmin\u001b[39m=\u001b[39;49mreturn_argmin,\n\u001b[1;32m    686\u001b[0m     show_progressbar\u001b[39m=\u001b[39;49mshow_progressbar,\n\u001b[1;32m    687\u001b[0m     early_stop_fn\u001b[39m=\u001b[39;49mearly_stop_fn,\n\u001b[1;32m    688\u001b[0m     trials_save_file\u001b[39m=\u001b[39;49mtrials_save_file,\n\u001b[1;32m    689\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda/envs/eda/lib/python3.11/site-packages/hyperopt/fmin.py:586\u001b[0m, in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    583\u001b[0m rval\u001b[39m.\u001b[39mcatch_eval_exceptions \u001b[39m=\u001b[39m catch_eval_exceptions\n\u001b[1;32m    585\u001b[0m \u001b[39m# next line is where the fmin is actually executed\u001b[39;00m\n\u001b[0;32m--> 586\u001b[0m rval\u001b[39m.\u001b[39;49mexhaust()\n\u001b[1;32m    588\u001b[0m \u001b[39mif\u001b[39;00m return_argmin:\n\u001b[1;32m    589\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(trials\u001b[39m.\u001b[39mtrials) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda/envs/eda/lib/python3.11/site-packages/hyperopt/fmin.py:364\u001b[0m, in \u001b[0;36mFMinIter.exhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexhaust\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    363\u001b[0m     n_done \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials)\n\u001b[0;32m--> 364\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_evals \u001b[39m-\u001b[39;49m n_done, block_until_done\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49masynchronous)\n\u001b[1;32m    365\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials\u001b[39m.\u001b[39mrefresh()\n\u001b[1;32m    366\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda/envs/eda/lib/python3.11/site-packages/hyperopt/fmin.py:300\u001b[0m, in \u001b[0;36mFMinIter.run\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    297\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpoll_interval_secs)\n\u001b[1;32m    298\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    299\u001b[0m     \u001b[39m# -- loop over trials and do the jobs directly\u001b[39;00m\n\u001b[0;32m--> 300\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mserial_evaluate()\n\u001b[1;32m    302\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials\u001b[39m.\u001b[39mrefresh()\n\u001b[1;32m    303\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials_save_file \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda/envs/eda/lib/python3.11/site-packages/hyperopt/fmin.py:178\u001b[0m, in \u001b[0;36mFMinIter.serial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    176\u001b[0m ctrl \u001b[39m=\u001b[39m base\u001b[39m.\u001b[39mCtrl(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials, current_trial\u001b[39m=\u001b[39mtrial)\n\u001b[1;32m    177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 178\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdomain\u001b[39m.\u001b[39;49mevaluate(spec, ctrl)\n\u001b[1;32m    179\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    180\u001b[0m     logger\u001b[39m.\u001b[39merror(\u001b[39m\"\u001b[39m\u001b[39mjob exception: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mstr\u001b[39m(e))\n",
      "File \u001b[0;32m~/miniconda/envs/eda/lib/python3.11/site-packages/hyperopt/base.py:892\u001b[0m, in \u001b[0;36mDomain.evaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    884\u001b[0m     \u001b[39m# -- the \"work\" of evaluating `config` can be written\u001b[39;00m\n\u001b[1;32m    885\u001b[0m     \u001b[39m#    either into the pyll part (self.expr)\u001b[39;00m\n\u001b[1;32m    886\u001b[0m     \u001b[39m#    or the normal Python part (self.fn)\u001b[39;00m\n\u001b[1;32m    887\u001b[0m     pyll_rval \u001b[39m=\u001b[39m pyll\u001b[39m.\u001b[39mrec_eval(\n\u001b[1;32m    888\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexpr,\n\u001b[1;32m    889\u001b[0m         memo\u001b[39m=\u001b[39mmemo,\n\u001b[1;32m    890\u001b[0m         print_node_on_error\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrec_eval_print_node_on_error,\n\u001b[1;32m    891\u001b[0m     )\n\u001b[0;32m--> 892\u001b[0m     rval \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfn(pyll_rval)\n\u001b[1;32m    894\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(rval, (\u001b[39mfloat\u001b[39m, \u001b[39mint\u001b[39m, np\u001b[39m.\u001b[39mnumber)):\n\u001b[1;32m    895\u001b[0m     dict_rval \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mfloat\u001b[39m(rval), \u001b[39m\"\u001b[39m\u001b[39mstatus\u001b[39m\u001b[39m\"\u001b[39m: STATUS_OK}\n",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mobjective\u001b[39m(params):\n\u001b[1;32m      2\u001b[0m     \u001b[39m# Create the RandomForestClassifier with the given hyperparameters\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     clf \u001b[39m=\u001b[39m CustomLogisticRegression(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)\n\u001b[0;32m----> 4\u001b[0m     clf\u001b[39m.\u001b[39;49mfit(X_train\u001b[39m.\u001b[39;49mvalues, y_train\u001b[39m.\u001b[39;49mvalues)\n\u001b[1;32m      6\u001b[0m     y_pred \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39mpredict(X_val\u001b[39m.\u001b[39mvalues)\n\u001b[1;32m      7\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39maccuracy_score(y_val, y_pred)\n",
      "File \u001b[0;32m~/Masters/IFT6390_FundamentalsOfML/Kaggle_competition/self_learn.py:111\u001b[0m, in \u001b[0;36mCustomLogisticRegression.fit\u001b[0;34m(self, X, y, X_val, y_val)\u001b[0m\n\u001b[1;32m    109\u001b[0m train_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss(y, probs)\n\u001b[1;32m    110\u001b[0m train_accuracy \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(y \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict(X))\n\u001b[0;32m--> 111\u001b[0m val_accuracy \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(y_val \u001b[39m==\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(X_val)) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalidation \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose:\n\u001b[1;32m    114\u001b[0m     \u001b[39m# Log metrics to wandb\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     wandb\u001b[39m.\u001b[39mlog({\n\u001b[1;32m    116\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m\"\u001b[39m: epoch,\n\u001b[1;32m    117\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtrain_loss\u001b[39m\u001b[39m\"\u001b[39m: train_loss,\n\u001b[1;32m    118\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtrain_accuracy\u001b[39m\u001b[39m\"\u001b[39m: train_accuracy,\n\u001b[1;32m    119\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mval_accuracy\u001b[39m\u001b[39m\"\u001b[39m: val_accuracy\n\u001b[1;32m    120\u001b[0m     })\n",
      "File \u001b[0;32m~/Masters/IFT6390_FundamentalsOfML/Kaggle_competition/self_learn.py:150\u001b[0m, in \u001b[0;36mCustomLogisticRegression.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[0;32m--> 150\u001b[0m     logits \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39;49mdot(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias\n\u001b[1;32m    151\u001b[0m     probs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msoftmax(logits)\n\u001b[1;32m    152\u001b[0m     output \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(probs, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'dot'"
     ]
    }
   ],
   "source": [
    "def objective(params):\n",
    "    # Create the RandomForestClassifier with the given hyperparameters\n",
    "    clf = CustomLogisticRegression(**params)\n",
    "    clf.fit(X_train.values, y_train.values)\n",
    "\n",
    "    y_pred = clf.predict(X_val.values)\n",
    "    loss = -accuracy_score(y_val, y_pred)\n",
    "\n",
    "    return {'loss': loss, 'status': STATUS_OK}\n",
    "\n",
    "# Define the search space for hyperparameters\n",
    "space = {\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.001), np.log(0.1)),\n",
    "    'num_epochs': hp.choice('num_epochs', [1000, 1500]),\n",
    "    'regularization': hp.choice('regularization', ['L1', 'L2']),\n",
    "    'lambda_reg': hp.uniform('lambda_reg', 0.001, 0.1),\n",
    "    'gamma': hp.uniform('gamma', 2.0, 3.0)\n",
    "}\n",
    "\n",
    "# Initialize Trials object to keep track of results\n",
    "trials = Trials()\n",
    "\n",
    "# Run the optimization\n",
    "best = fmin(\n",
    "    fn=objective,\n",
    "    space=space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=100,\n",
    "    trials=trials\n",
    ")\n",
    "\n",
    "print(\"Best hyperparameters:\", best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BaymaxNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 60/100 [26:01<19:43, 29.60s/trial, best loss: -0.803763440860215] "
     ]
    }
   ],
   "source": [
    "def objective(params):\n",
    "    # Create the RandomForestClassifier with the given hyperparameters\n",
    "    clf = RandomForestClassifier(**params)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_val)\n",
    "    loss = -accuracy_score(y_val, y_pred)\n",
    "\n",
    "    return {'loss': loss, 'status': STATUS_OK}\n",
    "\n",
    "# Define the search space for hyperparameters\n",
    "space = {\n",
    "    'n_estimators': hp.choice('n_estimators', range(100, 1000)),\n",
    "    'max_depth': hp.choice('max_depth', range(5, 50)),\n",
    "    'min_samples_split': hp.uniform('min_samples_split', 0.01, 0.1),\n",
    "    'min_samples_leaf': hp.uniform('min_samples_leaf', 0.01, 0.1),\n",
    "    'max_features': hp.choice('max_features', ['auto', 'sqrt', 'log2', None]),\n",
    "    'bootstrap': hp.choice('bootstrap', [True, False]),\n",
    "    'class_weight': hp.choice('class_weight', [None, 'balanced', 'balanced_subsample']),\n",
    "    'criterion': hp.choice('criterion', ['gini', 'entropy']),\n",
    "}\n",
    "\n",
    "# Initialize Trials object to keep track of results\n",
    "trials = Trials()\n",
    "\n",
    "# Run the optimization\n",
    "best = fmin(\n",
    "    fn=objective,\n",
    "    space=space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=100,\n",
    "    trials=trials\n",
    ")\n",
    "\n",
    "print(\"Best hyperparameters:\", best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [13:26<00:00,  8.06s/trial, best loss: -0.8190860215053763]\n",
      "Best hyperparameters: {'C': 6.898604855527303, 'class_weight': 0, 'l1_ratio': 0.7072602895829233, 'max_iter': 885, 'penalty': 0, 'solver': 0}\n"
     ]
    }
   ],
   "source": [
    "def objective(params):\n",
    "    # Create the RandomForestClassifier with the given hyperparameters\n",
    "    clf = LogisticRegression(**params)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_val)\n",
    "    loss = -accuracy_score(y_val, y_pred)\n",
    "\n",
    "    return {'loss': loss, 'status': STATUS_OK}\n",
    "\n",
    "# Define the search space for hyperparameters\n",
    "space = {\n",
    "    'C': hp.loguniform('C', np.log(1e-1), np.log(1e1)),  # Inverse of regularization strength\n",
    "    'solver': hp.choice('solver', ['saga']),  # Algorithm to use in the optimization problem\n",
    "    'max_iter': hp.choice('max_iter', range(100, 1000)),  # Maximum number of iterations taken for the solvers to converge\n",
    "    'penalty': hp.choice('penalty', ['l1', 'l2', 'elasticnet']),  # Used to specify the norm used in the penalization\n",
    "    'l1_ratio': hp.uniform('l1_ratio', 0, 1),  # The Elastic-Net mixing parameter, with 0 <= l1_ratio <= 1\n",
    "    'class_weight': hp.choice('class_weight', [None, 'balanced']),  # Weights associated with classes\n",
    "}\n",
    "\n",
    "# Initialize Trials object to keep track of results\n",
    "trials = Trials()\n",
    "\n",
    "# Run the optimization\n",
    "best = fmin(\n",
    "    fn=objective,\n",
    "    space=space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=100,\n",
    "    trials=trials\n",
    ")\n",
    "\n",
    "print(\"Best hyperparameters:\", best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [04:15<00:00,  2.56s/trial, best loss: -0.810752688172043]\n",
      "Best hyperparameters: {'colsample_bytree': 0.9385812500707504, 'gamma': 0.188570709579355, 'learning_rate': 0.17, 'max_delta_step': 5, 'max_depth': 9, 'min_child_weight': 7, 'n_estimators': 254, 'reg_alpha': 0.2791453005653505, 'reg_lambda': 2.9636012865585113, 'scale_pos_weight': 7.692974658470718, 'subsample': 0.9514650766399473}\n"
     ]
    }
   ],
   "source": [
    "def objective(params):\n",
    "    # Create the RandomForestClassifier with the given hyperparameters\n",
    "    clf = XGBClassifier(**params)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_val)\n",
    "    loss = -accuracy_score(y_val, y_pred)\n",
    "\n",
    "    return {'loss': loss, 'status': STATUS_OK}\n",
    "\n",
    "# Define the search space for hyperparameters\n",
    "space = {\n",
    "    'n_estimators': hp.choice('n_estimators', range(50, 500)),  # Number of gradient boosted trees\n",
    "    'learning_rate': hp.quniform('learning_rate', 0.01, 0.2, 0.01),  # Boosting learning rate\n",
    "    'max_depth': hp.choice('max_depth', range(3, 14)),  # Maximum tree depth for base learners\n",
    "    'min_child_weight': hp.choice('min_child_weight', range(1, 10)),  # Minimum sum of instance weight(hessian) needed in a child\n",
    "    'gamma': hp.uniform('gamma', 0.0, 0.5),  # Minimum loss reduction required to make a further partition on a leaf node of the tree\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1.0),  # Subsample ratio of the training instance\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0),  # Subsample ratio of columns when constructing each tree\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),  # L1 regularization term on weights\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 1.0, 4.0),  # L2 regularization term on weights\n",
    "    'scale_pos_weight': hp.uniform('scale_pos_weight', 1.0, 10.0),  # Balancing of positive and negative weights\n",
    "    'max_delta_step': hp.choice('max_delta_step', range(1, 10)),  # Maximum delta step we allow each tree's weight estimation to be\n",
    "    'objective': 'multi:softmax',  # Objective function for multiclass classification,\n",
    "}\n",
    "# Initialize Trials object to keep track of results\n",
    "trials = Trials()\n",
    "\n",
    "# Run the optimization\n",
    "best = fmin(\n",
    "    fn=objective,\n",
    "    space=space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=100,\n",
    "    trials=trials\n",
    ")\n",
    "\n",
    "print(\"Best hyperparameters:\", best)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
